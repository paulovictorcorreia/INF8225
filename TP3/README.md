# TP3 - Transformers and RNN

The third TP consists of building a translator machine from english to french using three different techniques: attention transformers, Vanilla Recurrent Neural Networks (RNN) and Gated Recurrent Unit (GRU) RNN.

We had to build from scratch using pytorch all the functions necessary to make these architectures work, such as multiheaded self-attention layers, positional encoding, transformer's encoder and decoder, and the recurrent units. The model's training was possible due to the autograd features of the pytorch module.

Finally, we made a report with the experiments, showing the limitations of the model coming from limited computational resources and the results obtained so far. As well, we present some possibilities of future work and display some training results.

## Files

* INF8225_TP3_H23.ipynb
* INF8225_TP3_H23_Report.pdf