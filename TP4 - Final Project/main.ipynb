{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "We are building a machine learning pipeline for classification of EEG signals.\n",
    "\n",
    "Preprocessing files will be run separetely from this notebook, and we will import their variables.\n",
    "\n",
    "This notebook will focus on creating the pipeline for assessing the best model to detect seizures in EEG signals. We will use three main strategies:\n",
    "\n",
    "* Res2Net Transformer\n",
    "* 1D-CNN + LSTM \n",
    "* Gated 2 Tower Transformer "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "2.0.0+cu118\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import lightning.pytorch as pl\n",
    "import pickle\n",
    "import copy\n",
    "\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.__version__)\n",
    "\n",
    "\n",
    "plt.style.use(\"ggplot\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing and Preprocessing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ./preprocessing.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = torch.FloatTensor(X_train)\n",
    "X_test = torch.FloatTensor(X_test)\n",
    "X_val = torch.FloatTensor(X_val)\n",
    "y_train = torch.FloatTensor(y_train).unsqueeze(1)\n",
    "y_test = torch.FloatTensor(y_test).unsqueeze(1)\n",
    "y_val = torch.FloatTensor(y_val).unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0., 1.]), tensor([6440, 1610]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.unique(return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EEGDataset(Dataset):\n",
    "    def __init__(self, features, target) -> None:\n",
    "        super().__init__()\n",
    "        self.features = features\n",
    "        self.target = target\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        data = {}\n",
    "        features = self.features[index]\n",
    "        target = self.target[index]\n",
    "        data[\"X\"] = features\n",
    "        data[\"y\"] = target\n",
    "        return data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "DEVICE = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "train_dataloader = DataLoader(EEGDataset(X_train, y_train), batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_dataloader = DataLoader(EEGDataset(X_val, y_val), batch_size=BATCH_SIZE, shuffle=False)\n",
    "final_train_dataloader = DataLoader(EEGDataset(torch.cat((X_train, X_val), 0), torch.cat((y_train, y_val), 0)), batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_dataloader = DataLoader(EEGDataset(X_test, y_test), batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Models"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def training(\n",
    "        model, train_dataloader=None, val_dataloader=None,\n",
    "        epochs=5, lr=0.001, device='cpu', earlystopping_tolerance=5):\n",
    "    model = model.to(device)\n",
    "    optimizer = Adam(model.parameters(), lr=lr)\n",
    "    # optimizer = torch.optim.Adadelta(model.parameters(), lr=lr)\n",
    "    criterion = nn.BCEWithLogitsLoss(pos_weight=torch.FloatTensor([9200/2300]).to(device))\n",
    "    # criterion = nn.BCELoss()\n",
    "    model_state = {\n",
    "        \"model\": None,\n",
    "        \"train_loss\": [],\n",
    "        \"val_loss\": [],\n",
    "    }\n",
    "    best_validation = np.inf\n",
    "    best_model = None\n",
    "    count_tolerance = 0\n",
    "\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        training_loss = 0\n",
    "        for i, data in enumerate(train_dataloader, 1):\n",
    "            # get the inputs; data is a list of [inputs, labels]\n",
    "            X, y = data[\"X\"].to(device), data[\"y\"].to(device)\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = model(X)\n",
    "            train_loss = criterion(outputs, y)\n",
    "            train_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # print statistics\n",
    "            training_loss += train_loss.item()\n",
    "        \n",
    "        training_loss /= i\n",
    "        \n",
    "        if isinstance(val_dataloader, DataLoader):\n",
    "            validation_loss = 0\n",
    "            for j, data in enumerate(val_dataloader, 1):\n",
    "                # get the inputs; data is a list of [inputs, labels]\n",
    "                X, y = data[\"X\"].to(device), data[\"y\"].to(device)\n",
    "\n",
    "\n",
    "                # forward + backward + optimize\n",
    "                with torch.no_grad():\n",
    "                    outputs = model(X)\n",
    "                    val_loss = criterion(outputs, y)\n",
    "                    # print statistics\n",
    "                    validation_loss += val_loss.item()\n",
    "\n",
    "            validation_loss /= j\n",
    "            \n",
    "            if (validation_loss) < best_validation:\n",
    "                count_tolerance = 0\n",
    "                best_validation = validation_loss\n",
    "                best_model = copy.deepcopy(model)\n",
    "            \n",
    "            count_tolerance += 1\n",
    "            print(f\"Epoch: {epoch}\\tTraining loss: {training_loss:.5f}\\t\\t Validation Loss: {validation_loss:.5f}\")\n",
    "            model_state[\"train_loss\"].append(training_loss)\n",
    "            model_state[\"val_loss\"].append(validation_loss)\n",
    "\n",
    "            if count_tolerance >= earlystopping_tolerance:\n",
    "                break\n",
    "            \n",
    "        else:\n",
    "            print(f\"Epoch: {epoch}\\tTraining loss: {training_loss:.5f}\")\n",
    "            model_state[\"train_loss\"].append(training_loss)\n",
    "            best_model = copy.deepcopy(model)\n",
    "        \n",
    "    \n",
    "    model_state[\"model\"] = best_model\n",
    "    save_model(model_state)\n",
    "    return model_state\n",
    "\n",
    "def save_model(model_state):\n",
    "    with open(f\"models/{ model_state['model'].to_string() }.pkl\", \"wb\") as fp:\n",
    "        model_state[\"model\"] = model_state[\"model\"].to(\"cpu\").state_dict()\n",
    "        pickle.dump(model_state, fp)\n",
    "        print(\"Saved model successfully!\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1D CNN-LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\tTraining loss: 1.10903\t\t Validation Loss: 1.10591\n",
      "Epoch: 1\tTraining loss: 1.04071\t\t Validation Loss: 0.97511\n",
      "Epoch: 2\tTraining loss: 0.91864\t\t Validation Loss: 0.90266\n",
      "Epoch: 3\tTraining loss: 0.86137\t\t Validation Loss: 0.88162\n",
      "Epoch: 4\tTraining loss: 0.83003\t\t Validation Loss: 0.84592\n",
      "Saved model successfully!\n"
     ]
    }
   ],
   "source": [
    "class CNN_LSTM_Classifier(pl.LightningModule):\n",
    "    def __init__(self,):\n",
    "        super().__init__()\n",
    "        self.device_ = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "        self.conv_1 = nn.Conv1d(1, 64, 3)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.max_pool = nn.MaxPool1d(2, 2)\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv1d(64, 128, 3),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(128, 512, 3),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(512, 1024, 3),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.flatten_layer = nn.Linear(82, 256)\n",
    "        dropout = 0.2\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.lstm = nn.LSTM(1024, 64, 2, batch_first=True, dropout=dropout)\n",
    "\n",
    "        self.fc_out = nn.Sequential(\n",
    "            nn.Linear(64, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1),\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, X: torch.FloatTensor, y=None):\n",
    "        X = X.transpose(1, 2)\n",
    "        out = self.relu(self.conv_1(X))\n",
    "        out = self.max_pool(out)\n",
    "        out = self.conv_layers(out)\n",
    "        out = self.flatten_layer(out)\n",
    "        out = out.transpose(1, 2)\n",
    "        out, (_, _) = self.lstm(out)\n",
    "        out = out[:, -1, :]\n",
    "        out = self.fc_out(out)\n",
    "        return out\n",
    "    \n",
    "    def predict_batch(self, X: torch.FloatTensor):\n",
    "        pred = (torch.sigmoid(self(X)) > 0.5).int()\n",
    "        return pred\n",
    "\n",
    "    def predict(self, dataloader: DataLoader):\n",
    "        predictions = list()\n",
    "        for i, data in enumerate(dataloader, 1):\n",
    "            # get the inputs; data is a list of [inputs, labels]\n",
    "            with torch.no_grad():\n",
    "                X = data[\"X\"]\n",
    "                y_pred = self.predict_batch(X)\n",
    "                predictions.append(y_pred)\n",
    "        predictions = torch.cat(predictions, 0)\n",
    "        return predictions\n",
    "\n",
    "    def to_string(self):\n",
    "        return \"CNN_LSTM_Classifier\"\n",
    "\n",
    "\n",
    "    \n",
    "model = CNN_LSTM_Classifier()\n",
    "state = training(model, train_dataloader, val_dataloader, device=DEVICE, epochs=5, lr=0.0001)\n",
    "# state[\"model\"]\n",
    "# model.predict(test_dataloader).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(8, 6))\n",
    "# plt.plot(state[\"train_loss\"], label=\"Training Loss\")\n",
    "# plt.plot(state[\"val_loss\"], label=\"Validation Loss\")\n",
    "# plt.legend(loc=\"best\")\n",
    "# plt.title(\"Training x Validation Losses\")\n",
    "# plt.xlabel(\"Epochs\")\n",
    "# plt.ylabel(\"Binary Cross Entropy Loss\")\n",
    "# plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Positional Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"\n",
    "    The authors of the original transformer paper describe very succinctly what \n",
    "    the positional encoding layer does and why it is needed:\n",
    "    \n",
    "    \"Since our model contains no recurrence and no convolution, in order for the \n",
    "    model to make use of the order of the sequence, we must inject some \n",
    "    information about the relative or absolute position of the tokens in the \n",
    "    sequence.\" (Vaswani et al, 2017)\n",
    "    Adapted from: \n",
    "    https://pytorch.org/tutorials/beginner/transformer_tutorial.html\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, \n",
    "        dropout: float=0.1, \n",
    "        max_seq_len: int=5000, \n",
    "        d_model: int=512,\n",
    "        batch_first: bool=False\n",
    "        ):\n",
    "\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "            dropout: the dropout rate\n",
    "            max_seq_len: the maximum length of the input sequences\n",
    "            d_model: The dimension of the output of sub-layers in the model \n",
    "                     (Vaswani et al, 2017)\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        self.batch_first = batch_first\n",
    "\n",
    "        self.x_dim = 1 if batch_first else 0\n",
    "\n",
    "        # copy pasted from PyTorch tutorial\n",
    "        position = torch.arange(max_seq_len).unsqueeze(1)\n",
    "        \n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        \n",
    "        pe = torch.zeros(max_seq_len, 1, d_model)\n",
    "        \n",
    "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "        \n",
    "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "        \n",
    "        self.register_buffer('pe', pe)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Tensor, shape [batch_size, enc_seq_len, dim_val] or \n",
    "               [enc_seq_len, batch_size, dim_val]\n",
    "        \"\"\"\n",
    "\n",
    "        x = x + self.pe[:x.size(self.x_dim)]\n",
    "\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gated Transformer Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\tTraining loss: 1.12035\t\t Validation Loss: 1.07689\n",
      "Epoch: 1\tTraining loss: 0.97601\t\t Validation Loss: 0.90811\n",
      "Epoch: 2\tTraining loss: 0.85421\t\t Validation Loss: 0.82152\n",
      "Epoch: 3\tTraining loss: 0.77639\t\t Validation Loss: 0.78973\n",
      "Epoch: 4\tTraining loss: 0.70246\t\t Validation Loss: 0.76238\n",
      "Saved model successfully!\n"
     ]
    }
   ],
   "source": [
    "class GatedTransformerNet(nn.Module):\n",
    "    def __init__(self, device=\"cpu\"):\n",
    "        super().__init__()\n",
    "        self.device = device \n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "\n",
    "        self.d_model = 512\n",
    "\n",
    "        self.step_embedding = nn.Linear(1, self.d_model)\n",
    "        self.channel_embedding = nn.Linear(1, self.d_model)\n",
    "        self.positional_embedding = PositionalEncoding(d_model=self.d_model, dropout=0.2)\n",
    "        self.tanh = nn.Tanh()\n",
    "\n",
    "        self.step_encoder = nn.TransformerEncoder(nn.TransformerEncoderLayer(self.d_model, nhead=8), num_layers=2)\n",
    "        self.channel_encoder = nn.TransformerEncoder(nn.TransformerEncoderLayer(self.d_model, nhead=8), num_layers=2)\n",
    "\n",
    "        self.gating = nn.Linear(self.d_model*178*2, 2)\n",
    "\n",
    "        self.fc_out = nn.Sequential(\n",
    "            nn.Linear(self.d_model*178*2, 256),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1),\n",
    "        )\n",
    "    \n",
    "    def generate_square_subsequent_mask(self, sz: int) -> torch.Tensor:\n",
    "        \"\"\"Generates an upper-triangular matrix of -inf, with zeros on diag.\"\"\"\n",
    "        return torch.triu(torch.ones(sz, sz) * float('-inf'), diagonal=1).to(self.device)\n",
    "\n",
    "\n",
    "    def forward(self, X: torch.FloatTensor, y=None):\n",
    "        batch_size = X.shape[0]\n",
    "        seq_len = X.shape[1]\n",
    "        mask = self.generate_square_subsequent_mask(batch_size)\n",
    "\n",
    "        channel = self.tanh(self.channel_embedding(X))\n",
    "        channel = self.dropout(self.channel_encoder(channel))\n",
    "\n",
    "        step = self.tanh(self.step_embedding(X))\n",
    "        step = self.positional_embedding(step)\n",
    "        step = self.dropout(self.step_encoder(step, mask))\n",
    "\n",
    "        channel = channel.reshape(batch_size, -1)\n",
    "        step = step.reshape(batch_size, -1)\n",
    "\n",
    "        concat = torch.cat([channel, step], -1)\n",
    "        h = self.gating(concat)\n",
    "        gate = torch.softmax(h, dim=-1)\n",
    "\n",
    "\n",
    "        encoding = torch.cat([channel * gate[:, 0:1], step * gate[:, 1:2]], dim=-1)\n",
    "        encoding = self.dropout(encoding)\n",
    "        out = self.fc_out(encoding)\n",
    "\n",
    "        return out\n",
    "    \n",
    "    def predict_batch(self, X: torch.FloatTensor):\n",
    "        pred = (torch.sigmoid(self(X)) > 0.5).int()\n",
    "        return pred\n",
    "\n",
    "    def predict(self, dataloader: DataLoader):\n",
    "        predictions = list()\n",
    "        for i, data in enumerate(dataloader, 1):\n",
    "            # get the inputs; data is a list of [inputs, labels]\n",
    "            with torch.no_grad():\n",
    "                X = data[\"X\"]\n",
    "                y_pred = self.predict_batch(X)\n",
    "                predictions.append(y_pred)\n",
    "        predictions = torch.cat(predictions, 0)\n",
    "        return predictions\n",
    "\n",
    "    def to_string(self):\n",
    "        return \"GatedTransformerNet\"\n",
    "\n",
    "model = GatedTransformerNet(device=DEVICE)\n",
    "state = training(model, train_dataloader, val_dataloader, device=DEVICE, epochs=5, lr=0.0001)\n",
    "# model(X_train[:32]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8050, 178, 1])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multilayer Perceptron Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\tTraining loss: 1.08846\t\t Validation Loss: 1.05065\n",
      "Epoch: 1\tTraining loss: 0.95194\t\t Validation Loss: 0.91078\n",
      "Epoch: 2\tTraining loss: 0.84612\t\t Validation Loss: 0.82710\n",
      "Epoch: 3\tTraining loss: 0.79448\t\t Validation Loss: 0.81759\n",
      "Epoch: 4\tTraining loss: 0.76118\t\t Validation Loss: 0.78374\n",
      "Saved model successfully!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0]], device='cuda:0', dtype=torch.int32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MLPClassifier(nn.Module):\n",
    "    def __init__(self, device=\"cpu\"):\n",
    "        super().__init__()\n",
    "        self.device_ = device\n",
    "\n",
    "        self.dropout_1 = nn.Dropout(0.1)\n",
    "\n",
    "        self.fc_net = nn.Sequential(\n",
    "            nn.Linear(178, 500),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(500, 500),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(500, 500),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(500, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, X: torch.Tensor) -> torch.Tensor:\n",
    "        X = X.squeeze(-1).to(self.device_)\n",
    "        X = self.dropout_1(X)\n",
    "        return self.fc_net(X)\n",
    "    \n",
    "    def predict_batch(self, X: torch.FloatTensor):\n",
    "        pred = (torch.sigmoid(self(X)) > 0.5).int()\n",
    "        return pred\n",
    "\n",
    "    def predict(self, dataloader: DataLoader):\n",
    "        predictions = list()\n",
    "        for i, data in enumerate(dataloader, 1):\n",
    "            # get the inputs; data is a list of [inputs, labels]\n",
    "            with torch.no_grad():\n",
    "                X = data[\"X\"]\n",
    "                y_pred = self.predict_batch(X)\n",
    "                predictions.append(y_pred)\n",
    "        predictions = torch.cat(predictions, 0)\n",
    "        return predictions\n",
    "\n",
    "    def to_string(self):\n",
    "        return \"MLPClassifier\"\n",
    "    \n",
    "\n",
    "model = MLPClassifier(DEVICE)\n",
    "state = training(model, train_dataloader, val_dataloader, device=DEVICE, epochs=5, lr=1e-4, earlystopping_tolerance=10)\n",
    "model.predict_batch(X_train[:32])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FCN (Fully Convoluted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\tTraining loss: 0.78734\t\t Validation Loss: 0.62891\n",
      "Epoch: 1\tTraining loss: 0.56710\t\t Validation Loss: 0.52135\n",
      "Epoch: 2\tTraining loss: 0.47348\t\t Validation Loss: 0.44257\n",
      "Epoch: 3\tTraining loss: 0.41519\t\t Validation Loss: 0.38716\n",
      "Epoch: 4\tTraining loss: 0.37916\t\t Validation Loss: 0.36255\n",
      "Epoch: 5\tTraining loss: 0.34183\t\t Validation Loss: 0.36619\n",
      "Epoch: 6\tTraining loss: 0.32938\t\t Validation Loss: 0.31104\n",
      "Epoch: 7\tTraining loss: 0.31243\t\t Validation Loss: 0.31013\n",
      "Epoch: 8\tTraining loss: 0.29606\t\t Validation Loss: 0.29248\n",
      "Epoch: 9\tTraining loss: 0.28701\t\t Validation Loss: 0.28065\n",
      "Saved model successfully!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 1])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class FCN(nn.Module):\n",
    "    def __init__(self, device=\"cpu\"):\n",
    "        super().__init__()\n",
    "        self.device_ = device\n",
    "\n",
    "        self.conv_fc = nn.Sequential(\n",
    "            nn.Conv1d(1, 128, 3),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(128, 256, 3),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(256, 128, 3),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.AdaptiveAvgPool1d(1),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(128, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, X):\n",
    "        X = X.transpose(-1, -2).to(self.device_)\n",
    "        conv = self.conv_fc(X)\n",
    "        return conv\n",
    "    \n",
    "    def predict_batch(self, X: torch.FloatTensor):\n",
    "        pred = (torch.sigmoid(self(X)) > 0.5).int()\n",
    "        return pred\n",
    "\n",
    "    def predict(self, dataloader: DataLoader):\n",
    "        predictions = list()\n",
    "        for i, data in enumerate(dataloader, 1):\n",
    "            # get the inputs; data is a list of [inputs, labels]\n",
    "            with torch.no_grad():\n",
    "                X = data[\"X\"]\n",
    "                y_pred = self.predict_batch(X)\n",
    "                predictions.append(y_pred)\n",
    "        predictions = torch.cat(predictions, 0)\n",
    "        return predictions\n",
    "\n",
    "    def to_string(self):\n",
    "        return \"FCN\"\n",
    "    \n",
    "\n",
    "model = FCN(DEVICE)\n",
    "state = training(model, train_dataloader, val_dataloader, device=DEVICE, epochs=10, lr=1e-4, earlystopping_tolerance=10)\n",
    "\n",
    "model(X_train[:32]).shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Residual Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 64, 178])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class ResNet(nn.Module):\n",
    "    def __init__(self, device=\"cpu\"):\n",
    "        super().__init__()\n",
    "        self.device_ = device\n",
    "\n",
    "        self.conv_block_1 = nn.Sequential(\n",
    "            nn.Conv1d(1, 64, 3, padding=1),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(64, 64, 3, padding=1),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(64, 64, 3, padding=1),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "    def forward(self, X):\n",
    "        X = X.transpose(-1, -2).to(self.device_)\n",
    "        conv = self.conv_block_1(X)\n",
    "        return conv\n",
    "    \n",
    "\n",
    "model = ResNet()\n",
    "# state = training(model, train_dataloader, val_dataloader, device=DEVICE, epochs=10, lr=1e-4, earlystopping_tolerance=10)\n",
    "\n",
    "model(X_train[:32]).shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "inf8225",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "89c218e4cc41e884f86683685cebc6d3a872232fe925e4ab554e8abe484c931d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
